{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb561af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mat73\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import savemat\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, roc_curve, auc, f1_score, accuracy_score, balanced_accuracy_score, cohen_kappa_score, recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.base import clone\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit, cross_val_score\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib qt\n",
    "import seaborn as sns\n",
    "from itertools import cycle\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler\n",
    "from imblearn.under_sampling import TomekLinks, RandomUnderSampler\n",
    "from collections import Counter\n",
    "import xlsxwriter\n",
    "import graphviz\n",
    "from imblearn.under_sampling import TomekLinks, RandomUnderSampler\n",
    "import random\n",
    "import time\n",
    "from matplotlib.lines import Line2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52864d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IntraIndividualBuilder(df, mm, patient_leftout):\n",
    "    patient_id = patient_leftout - 1\n",
    "    first_fallen = int(sum(mm[0 : patient_id]))\n",
    "    last_fallen = int(first_fallen + mm[patient_id])\n",
    "    Xytest = df.iloc[first_fallen : last_fallen, :]\n",
    "    Xytrain = df.drop(df.index[first_fallen : last_fallen], axis = 0)\n",
    "    return Xytest, Xytrain;\n",
    "\n",
    "def OnlySomeBuilder(df, mm, patients_array):\n",
    "    Train = pd.DataFrame()\n",
    "    summ = 0\n",
    "    mm_train = np.zeros((1, len(patients_array)))\n",
    "    for i in range(len(patients_array)):\n",
    "        ii = patients_array[i]\n",
    "        Test_i, _ = IntraIndividualBuilder(df, mm, ii)\n",
    "        Train = pd.concat([Train, Test_i], axis = 0)\n",
    "        summ += mm[ii]\n",
    "        mm_train[0, i] = int(mm[ii])\n",
    "        if Train.shape[0] != int(summ):\n",
    "            raise NameError('Error in dimensions')\n",
    "    return Train, mm_train;\n",
    "\n",
    "def SequentialLearner(df, mm, patient_leftout):\n",
    "    model = xgb.XGBClassifier(tree_method = 'exact',\n",
    "                              learning_rate = 0.01,\n",
    "                              objective = 'multi:softmax',\n",
    "                              use_label_encoder = False,\n",
    "                              eval_metric = ['mlogloss', 'auc'],\n",
    "                              verbosity = 0,\n",
    "                              max_depth = 10,\n",
    "                            )\n",
    "    for i in range(len(mm)):\n",
    "        if i != patient_leftout:\n",
    "            test, _ = IntraIndividualBuilder(df, mm, i)\n",
    "            print('Iteration:', i + 1)\n",
    "        else:\n",
    "            print('Skip patient')\n",
    "        sample_weights = class_weight.compute_sample_weight(class_weight='balanced',\n",
    "                                                            y = test.iloc[:, -1]\n",
    "                                                           )\n",
    "        model.fit(test.drop(columns = ['Sleep Stages']), test.iloc[:, -1], sample_weight = sample_weights)\n",
    "    return model;\n",
    "\n",
    "def OrdinalTrain(model_container, features, cardinal_labels, smote_options):\n",
    "    header_entries = list(cardinal_labels.columns.values)\n",
    "    for k in range(len(header_entries)):\n",
    "        cik = header_entries[k]\n",
    "        label_k = cardinal_3labels[cik]\n",
    "        \n",
    "        if smote_options == True:\n",
    "            sm = SMOTE(random_state = 42)\n",
    "            Xsm, label_sm = sm.fit_resample(features, label_k)\n",
    "            model_container[k].fit(Xsm, label_sm)\n",
    "        else:\n",
    "            sample_weights = class_weight.compute_sample_weight(class_weight = 'balanced',\n",
    "                                                                y = label_k,\n",
    "                                                               )\n",
    "            model_container[k].fit(features, label_k, sample_weight = sample_weights)\n",
    "    return model_container;\n",
    "\n",
    "def First2Hours(train_df):\n",
    "    last_index = 2 * 60 * 2 - 1\n",
    "    Xytrain = train_df.iloc[0 : last_index, :]\n",
    "    Xytest = train_df.drop(train_df.index[0 : last_index], axis = 0)\n",
    "    return Xytrain, Xytest;\n",
    "\n",
    "def Evaluation(predictions, n_class):\n",
    "    prob_array = np.zeros((n_class,))\n",
    "    epochs = len(predictions)\n",
    "    counted_labels = Counter(predictions)\n",
    "    for key in counted_labels:\n",
    "        if key == 0.0:\n",
    "            prob_array[0] = counted_labels[key] / epochs\n",
    "        elif key == 1.0:\n",
    "            prob_array[1] = counted_labels[key] / epochs\n",
    "        elif key == 2.0:\n",
    "            prob_array[2] = counted_labels[key] / epochs\n",
    "    return prob_array;\n",
    "\n",
    "def OSAEvaluation(predictions, truelabels):\n",
    "    truth_array = np.zeros((truelabels.size,))\n",
    "    for i in range(truelabels.size):\n",
    "        if truelabels[i] == 0:\n",
    "            if predictions[i] == 0:\n",
    "                truth_array[i] = 1\n",
    "        elif (truelabels[i] == 1) or (truelabels[i] == 2):\n",
    "            if (predictions[i] == 1) or (predictions[i] == 2):\n",
    "                truth_array[i] = 1\n",
    "    return truth_array;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d4cb629",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'c:/Users/Jacopo/Desktop/Paper - EDA Sleep Staging/Code Matlab/Keep it Simple/'\n",
    "os.chdir(path)\n",
    "\n",
    "DataOSA = mat73.loadmat('FinalOSAPy.mat')\n",
    "Data = mat73.loadmat('MegaStagePy.mat')\n",
    "Table = mat73.loadmat('FinalTablePy.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99134161",
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = DataOSA['mm']\n",
    "\n",
    "#Xn = DataOSA['X_alt']\n",
    "#Xns = DataOSA['Xns']\n",
    "\n",
    "#y = DataOSA['Yalt']\n",
    "#y2 = DataOSA['Yalt2']\n",
    "\n",
    "Xn = Data['Xss']\n",
    "mm = Data['mm']\n",
    "y5 = Data['Y5']\n",
    "\n",
    "y_AHI = DataOSA['y_AHI']\n",
    "y_ODI = DataOSA['y_ODI']\n",
    "\n",
    "names_features = ['Mode rawEDA', 'Median rawEDA', 'max(abs) rawEDA', 'Line length rawEDA', '10% quantile rawEDA', '75% quantile rawEDA',\n",
    "                'SVD entropy rawEDA',\n",
    "                'Nonlinear energy rawEDA', 'Shannon entropy rawEDA',\n",
    "                'Mode detEDA', 'Median detEDA', 'max(abs) detEDA', 'Line length detEDA', '10% quantile detEDA', '75% quantile detEDA',\n",
    "                'SVD entropy detEDA', 'Nonlinear Energy detEDA', 'Shannon entropy detEDA',\n",
    "                'Mean d(rawEDA)', 'Variance d(rawEDA)', 'Median d(rawEDA)', 'Number above zero d(rawEDA)',\n",
    "                'Mean d2(rawEDA)', 'Variance d2(rawEDA)', 'Median d2(rawEDA)', 'Number above zero d2(rawEDA)',\n",
    "                'Mean d(detEDA)', 'Variance d(detEDA)', 'Median d(detEDA)', 'Number above zero d(detEDA)',\n",
    "                'Mean d2(detEDA)', 'Variance d2(detEDA)', 'Median d2(detEDA)', 'Number above zero d2(detEDA)',\n",
    "                'Max Periodogram rawEDA', 'Frequency of max PSD rawEDA', 'Fisher g ratio rawEDA',\n",
    "                'Max Periodogram detEDA', 'Frequency of max PSD detEDA', 'Fisher g ratio detEDA',\n",
    "                'Max det. coeff. DL 1',\n",
    "                'Mean det. coeff. DL 1', 'Standard deviation det. coeff. DL 1',\n",
    "                'Median det. coeff. DL 1', '2-norm det. coeff. DL 1',\n",
    "                'Normalised det. coeff. above zero DL 1',\n",
    "                'Max det. coeff. DL 2', 'Mean det. coeff. DL 2',\n",
    "                'Standard deviation det. coeff. DL 2', 'Median det. coeff. DL 2',\n",
    "                '2-norm det. coeff. DL 2',\n",
    "                'Normalised det. coeff. above zero DL 2',\n",
    "                'Max det. coeff. DL 3', 'Mean det. coeff. DL 3',\n",
    "                'Standard deviation det. coeff. DL 3', 'Median det. coeff. DL 3',\n",
    "                '2-norm det. coeff. above zero DL 3',\n",
    "                'Normalised det. coeff. above zero DL 3',\n",
    "                'Max det. coeff. DL 4', 'Mean det. coeff. DL 4',\n",
    "                'Standard deviation det. coeff. DL 4', 'Median det. coeff. DL 4',\n",
    "                '2-norm det. coeff. DL 4',\n",
    "                'Normalised det. coeff. above zero DL 4',\n",
    "                'Lyapunov exponent rawEDA', 'max(upper envelope) rawEDA', 'min(lower envelope) rawEDA',\n",
    "                'Lyapunov exponent detEDA', 'max(upper envelope) detEDA', 'min(lower envelope) detEDA',\n",
    "                'Sum of correlation between epochs', 'Max convolution value diffEDA',\n",
    "                'Normalised event samples', 'Normalised event energy', 'Normalised storm samples', 'Normalised storm energy',\n",
    "                'Sex'\n",
    "               ]\n",
    "Xnpd = pd.DataFrame(Xn, columns = names_features)\n",
    "map_3 = {'Wake' : 0, 'N1' : 1, 'N2' : 1, 'N3' : 1, 'REM' : 2}\n",
    "map_4 = {'Wake' : 0, 'N1' : 1, 'N2' : 1, 'N3' : 2, 'REM' : 3}\n",
    "map_5 = {'Wake' : 0, 'N1' : 1, 'N2' : 2, 'N3' : 3, 'REM' : 4}\n",
    "map_sp = {'Wake' : 0, 'N1' : 1, 'N2' : 2, 'N3' : 3, 'REM' : 0}\n",
    "n_class = 3\n",
    "\n",
    "y_general = pd.DataFrame(y5, columns = ['Sleep Stages'])\n",
    "map_sleep = {0.0 : 'Wake', 1.0 : 'N1', 2.0 : 'N2', 3.0 : 'N3', 4.0 : 'REM'}\n",
    "y_general['Sleep Stages'] = y_general['Sleep Stages'].map(map_sleep)\n",
    "ypd = pd.DataFrame(columns = ['Sleep Stages'])\n",
    "classes_sp = ['W/REM', 'N1', 'N2', 'N3']\n",
    "\n",
    "if n_class == 5:\n",
    "    classes = ['Wale', 'N1', 'N2', 'N3', 'REM']\n",
    "    ypd['Sleep Stages'] = y_general['Sleep Stages'].map(map_5)\n",
    "elif n_class == 4:\n",
    "    classes = ['Wake', 'Light', 'Deep', 'REM']\n",
    "    ypd['Sleep Stages'] = y_general['Sleep Stages'].map(map_4)\n",
    "elif n_class == 3:\n",
    "    classes = ['Wake', 'NREM', 'REM']\n",
    "    ypd['Sleep Stages'] = y_general['Sleep Stages'].map(map_3)\n",
    "\n",
    "Xypd = pd.concat([Xnpd, ypd], axis = 1)\n",
    "\n",
    "standard = xgb.XGBClassifier(tree_method = 'hist',\n",
    "                                     learning_rate = 0.2,\n",
    "                                     max_depth = 6,\n",
    "                                     num_class = n_class,\n",
    "                                     eval_metric = ['mlogloss', 'auc'],\n",
    "                                     objective = 'multi:softmax',\n",
    "                                     use_label_encoder = False,\n",
    "                                     n_estimators = 200,\n",
    "                                    )\n",
    "\n",
    "Yahipd = pd.DataFrame(y_AHI, columns = ['OSA'])\n",
    "Yodipd = pd.DataFrame(y_ODI, columns = ['OSA'])\n",
    "\n",
    "classes_OSA = ['Healthy', 'Mild OSA', 'Severe OSA']\n",
    "    \n",
    "Xypd_ODI = pd.concat([Xnpd, Yodipd], axis = 1)\n",
    "Xypd_AHI = pd.concat([Xnpd, Yahipd], axis = 1)\n",
    "Diff = Table['Diff']\n",
    "AHI = Table['AHI']\n",
    "ODI = Table['ODI']\n",
    "n_classOSA = 3\n",
    "#classes_OSA = ['Non-OSA', 'OSA']\n",
    "\n",
    "yODI2 = pd.DataFrame(y_ODI, columns = ['OSA'])\n",
    "map2 = {0.0 : 0, 1.0 : 1, 2.0 : 1}\n",
    "yODI2['OSA'] = yODI2['OSA'].map(map2)\n",
    "\n",
    "yAHI2 = pd.DataFrame(y_AHI, columns = ['OSA'])\n",
    "yAHI2['OSA'] = yAHI2['OSA'].map(map2)\n",
    "\n",
    "Xypd_ODI2 = pd.concat([Xnpd, yODI2], axis = 1)\n",
    "Xypd_AHI2 = pd.concat([Xnpd, yAHI2], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "273dc253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mode rawEDA                 0.709764\n",
       "Median rawEDA               0.692975\n",
       "max(abs) rawEDA             0.582992\n",
       "Line length rawEDA          0.015914\n",
       "10% quantile rawEDA         0.720288\n",
       "                              ...   \n",
       "Normalised event samples         NaN\n",
       "Normalised event energy          NaN\n",
       "Normalised storm samples         NaN\n",
       "Normalised storm energy          NaN\n",
       "Sex                              NaN\n",
       "Name: 10% quantile detEDA, Length: 77, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cor_matrix = Xnpd.corr().abs()\n",
    "upper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape), k = 1).astype(bool))\n",
    "upper_tri['10% quantile detEDA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72fb7ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "['Median rawEDA', 'max(abs) rawEDA', '10% quantile rawEDA', '75% quantile rawEDA', 'Shannon entropy rawEDA', 'Median detEDA', '10% quantile detEDA', '75% quantile detEDA', 'Shannon entropy detEDA', 'Variance d2(rawEDA)', 'Number above zero d2(rawEDA)', 'Variance d(detEDA)', 'Number above zero d(detEDA)', 'Mean d2(detEDA)', 'Variance d2(detEDA)', 'Number above zero d2(detEDA)', 'Max Periodogram detEDA', 'Max det. coeff. DL 1', 'Mean det. coeff. DL 1', 'Standard deviation det. coeff. DL 1', '2-norm det. coeff. DL 1', 'Max det. coeff. DL 2', 'Mean det. coeff. DL 2', 'Standard deviation det. coeff. DL 2', '2-norm det. coeff. DL 2', 'Max det. coeff. DL 3', 'Mean det. coeff. DL 3', 'Standard deviation det. coeff. DL 3', '2-norm det. coeff. above zero DL 3', 'Max det. coeff. DL 4', 'Mean det. coeff. DL 4', 'Standard deviation det. coeff. DL 4', '2-norm det. coeff. DL 4', 'max(upper envelope) rawEDA', 'min(lower envelope) rawEDA', 'max(upper envelope) detEDA', 'min(lower envelope) detEDA']\n"
     ]
    }
   ],
   "source": [
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > .8)]\n",
    "print(len(to_drop))\n",
    "print(to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "15e4a57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "Index(['Mode rawEDA', 'Line length rawEDA', 'SVD entropy rawEDA',\n",
      "       'Nonlinear energy rawEDA', 'Mode detEDA', 'max(abs) detEDA',\n",
      "       'Line length detEDA', 'SVD entropy detEDA', 'Nonlinear Energy detEDA',\n",
      "       'Mean d(rawEDA)', 'Variance d(rawEDA)', 'Median d(rawEDA)',\n",
      "       'Number above zero d(rawEDA)', 'Mean d2(rawEDA)', 'Median d2(rawEDA)',\n",
      "       'Mean d(detEDA)', 'Median d(detEDA)', 'Median d2(detEDA)',\n",
      "       'Max Periodogram rawEDA', 'Frequency of max PSD rawEDA',\n",
      "       'Fisher g ratio rawEDA', 'Frequency of max PSD detEDA',\n",
      "       'Fisher g ratio detEDA', 'Median det. coeff. DL 1',\n",
      "       'Normalised det. coeff. above zero DL 1', 'Median det. coeff. DL 2',\n",
      "       'Normalised det. coeff. above zero DL 2', 'Median det. coeff. DL 3',\n",
      "       'Normalised det. coeff. above zero DL 3', 'Median det. coeff. DL 4',\n",
      "       'Normalised det. coeff. above zero DL 4', 'Lyapunov exponent rawEDA',\n",
      "       'Lyapunov exponent detEDA', 'Sum of correlation between epochs',\n",
      "       'Max convolution value diffEDA', 'Normalised event samples',\n",
      "       'Normalised event energy', 'Normalised storm samples',\n",
      "       'Normalised storm energy', 'Sex', 'OSA'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "Xysrpd = Xypd.drop(columns = to_drop, axis = 1)\n",
    "XysrODI = Xypd_ODI.drop(columns = to_drop, axis = 1)\n",
    "XysrAHI = Xypd_AHI.drop(columns = to_drop, axis = 1)\n",
    "XysrAHI2 = Xypd_AHI2.drop(columns = to_drop, axis = 1)\n",
    "XysrODI2 = Xypd_ODI2.drop(columns = to_drop, axis = 1)\n",
    "number_reduced = XysrODI.shape[1] - 1\n",
    "print(number_reduced)\n",
    "#XysrODI2\n",
    "print(XysrODI.columns)\n",
    "#sleep = XysrODI.columns\n",
    "osa = XysrODI.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ba7d7590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "20.808088064193726\n",
      "2\n",
      "20.18749213218689\n",
      "3\n",
      "20.804983139038086\n",
      "4\n",
      "20.91813063621521\n",
      "5\n",
      "20.831027507781982\n",
      "6\n",
      "21.157108545303345\n",
      "7\n",
      "21.304754972457886\n",
      "8\n",
      "21.12535548210144\n",
      "9\n",
      "20.818381309509277\n",
      "10\n",
      "21.923776388168335\n",
      "11\n",
      "20.919044971466064\n",
      "12\n",
      "21.16933250427246\n",
      "13\n",
      "21.76720929145813\n",
      "14\n",
      "21.448274612426758\n",
      "15\n",
      "21.556912422180176\n",
      "16\n",
      "20.901105165481567\n",
      "17\n",
      "20.75359797477722\n",
      "18\n",
      "20.463568449020386\n",
      "19\n",
      "21.407569408416748\n",
      "20\n",
      "20.655099391937256\n",
      "21\n",
      "21.304754734039307\n",
      "22\n",
      "23.06016731262207\n",
      "23\n",
      "21.283825159072876\n",
      "24\n",
      "20.723698139190674\n",
      "25\n",
      "20.878410816192627\n",
      "26\n",
      "22.104082107543945\n",
      "27\n",
      "22.496932983398438\n",
      "28\n",
      "21.28482174873352\n",
      "29\n",
      "21.478320121765137\n",
      "30\n",
      "20.564749717712402\n",
      "31\n",
      "21.048611402511597\n",
      "32\n",
      "21.932834148406982\n",
      "33\n",
      "21.531610012054443\n",
      "34\n",
      "20.927137851715088\n",
      "35\n",
      "21.171722173690796\n",
      "36\n",
      "21.35125970840454\n",
      "37\n",
      "21.695448875427246\n",
      "38\n",
      "21.49910593032837\n",
      "39\n",
      "21.66460418701172\n",
      "40\n",
      "21.37259793281555\n",
      "41\n",
      "21.45036792755127\n",
      "42\n",
      "21.537975549697876\n",
      "43\n",
      "21.081501007080078\n",
      "44\n",
      "21.140305995941162\n",
      "45\n",
      "21.329671621322632\n",
      "46\n",
      "21.69544792175293\n",
      "47\n",
      "21.425352096557617\n",
      "48\n",
      "21.48624563217163\n",
      "49\n",
      "20.969874620437622\n",
      "50\n",
      "21.36555242538452\n",
      "51\n",
      "21.347611904144287\n",
      "52\n",
      "20.763564586639404\n",
      "53\n",
      "20.860241413116455\n",
      "54\n",
      "21.345731019973755\n",
      "55\n",
      "21.247944831848145\n",
      "56\n",
      "21.43531847000122\n",
      "57\n",
      "22.27959108352661\n",
      "58\n",
      "21.900827646255493\n",
      "59\n",
      "21.2838716506958\n",
      "60\n",
      "21.860496520996094\n"
     ]
    }
   ],
   "source": [
    "CM_test = np.zeros((n_class, n_class))\n",
    "CM_test2 = np.zeros((n_class, n_class))\n",
    "\n",
    "features_summary = np.zeros((len(mm), number_reduced))\n",
    "features_summary2 = np.zeros((len(mm), number_reduced))\n",
    "features_diff = np.zeros((len(mm), number_reduced))\n",
    "scores = np.zeros((len(mm), 5))\n",
    "scores2 = np.zeros((len(mm), 5))\n",
    "train_sizes = np.zeros((len(mm),))\n",
    "sm = SMOTE(random_state = 42)\n",
    "train_size_increased = 0.15\n",
    "\n",
    "scores_xgb = np.zeros((len(mm), 8))\n",
    "mat_predictions_xgb_ODI = np.zeros((len(mm), len(classes_OSA)))\n",
    "features_xgb_train_ODI = np.zeros((len(mm), number_reduced))\n",
    "mat_predictions_xgb_AHI = np.zeros((len(mm), len(classes_OSA)))\n",
    "features_xgb_train_AHI = np.zeros((len(mm), number_reduced))\n",
    "CM_AHI = np.zeros((len(classes_OSA), len(classes_OSA)))\n",
    "CM_ODI = np.zeros((len(classes_OSA), len(classes_OSA)))\n",
    "AUC_AHI = np.zeros((len(mm), 3))\n",
    "AUC_ODI = np.zeros((len(mm), 3))\n",
    "predictions_AHI = np.array([])\n",
    "predictions_ODI = np.array([])\n",
    "true_AHI = np.array([])\n",
    "true_ODI = np.array([])\n",
    "\n",
    "for k in range(len(mm)):\n",
    "    print(k + 1)\n",
    "    start = time.time()\n",
    "    #test_k, train_k = IntraIndividualBuilder(Xysrpd, mm, k)\n",
    "    \n",
    "    #test_train_k, test_test_k = train_test_split(test_k, train_size = 0.25, random_state = 42)\n",
    "\n",
    "    #Xtrain_sm, Ytrain_sm = sm.fit_resample(train_k.drop(columns = ['Sleep Stages']), train_k.iloc[:, -1])\n",
    "    #Xtrain2_sm, Ytrain2_sm = sm.fit_resample(test_train_k.drop(columns = ['Sleep Stages']), test_train_k.iloc[:, -1])\n",
    "    \n",
    "    #Mdl_k = clone(standard)\n",
    "    #Mdl_k.fit(Xtrain_sm, Ytrain_sm)\n",
    "    #predictions_test = Mdl_k.predict(test_k.drop(columns = ['Sleep Stages']))\n",
    "    \n",
    "    #Mdl_k2 = clone(Mdl_k)\n",
    "    \n",
    "\n",
    "    #train2, test2 = First2Hours(test_k)    \n",
    "    \n",
    "    #Mdl_k2.fit(Xtrain2_sm, Ytrain2_sm)\n",
    "    #predictions_test2 = Mdl_k2.predict(test_test_k.drop(columns = ['Sleep Stages']))\n",
    "    #CM_test2 += confusion_matrix(test_test_k.iloc[:, -1], predictions_test2)\n",
    "    \n",
    "    #Mdl_k2.fit(train2.drop(columns = ['Sleep Stages']), train2.iloc[:, -1])\n",
    "    #predictions_test2 = Mdl_k2.predict(test2.drop(columns = ['Sleep Stages']))\n",
    "   \n",
    "    \n",
    "    #CM_test += confusion_matrix(test_k.iloc[:, -1], predictions_test)\n",
    "    #CM_test2 += confusion_matrix(test2.iloc[:, -1], predictions_test2)\n",
    "    \n",
    "    \n",
    "    #scores[k, 0] = f1_score(test_k.iloc[:, -1], predictions_test, average = 'macro')\n",
    "    #scores[k, 1] = f1_score(test_k.iloc[:, -1], predictions_test, average = 'micro')\n",
    "    #scores[k, 2] = cohen_kappa_score(test_k.iloc[:, -1], predictions_test)\n",
    "    #scores[k, 3] = recall_score(test_k.iloc[:, -1], predictions_test, average = 'macro')\n",
    "    #scores[k, 4] = recall_score(test_k.iloc[:, -1], predictions_test, average = 'micro')\n",
    "    \n",
    "    #scores2[k, 0] = f1_score(test_test_k.iloc[:, -1], predictions_test2, average = 'macro')\n",
    "    #scores2[k, 1] = f1_score(test_test_k.iloc[:, -1], predictions_test2, average = 'micro')\n",
    "    #scores2[k, 2] = cohen_kappa_score(test_test_k.iloc[:, -1], predictions_test2)\n",
    "    #scores2[k, 3] = recall_score(test_test_k.iloc[:, -1], predictions_test2, average = 'macro')\n",
    "    #scores2[k, 0] = f1_score(test2.iloc[:, -1], predictions_test2, average = 'macro')\n",
    "    #scores2[k, 1] = f1_score(test2.iloc[:, -1], predictions_test2, average = 'micro')\n",
    "    #scores2[k, 2] = cohen_kappa_score(test2.iloc[:, -1], predictions_test2)\n",
    "    #scores2[k, 3] = recall_score(test2.iloc[:, -1], predictions_test2, average = 'macro')\n",
    "\n",
    "    \n",
    "    #features_summary[k, :] = Mdl_k.feature_importances_\n",
    "    #features_summary2[k, :] = Mdl_k2.feature_importances_\n",
    "    #features_diff[k, :] = Mdl_k.feature_importances_ - Mdl_k2.feature_importances_\n",
    "\n",
    "    #######################\n",
    "    \n",
    "    test_AHI, train_AHI = IntraIndividualBuilder(XysrAHI, mm, k)\n",
    "    test_ODI, train_ODI = IntraIndividualBuilder(XysrODI, mm, k)\n",
    "            \n",
    "    Mdl_xgb_AHI = xgb.XGBClassifier(tree_method = 'hist',\n",
    "                                learning_rate = 0.01,\n",
    "                                #objective = 'binary:logistic',\n",
    "                                objective = 'multi:softmax',\n",
    "                                use_label_encoder = False,\n",
    "                                verbosity = 0,\n",
    "                                max_depth = 6,\n",
    "                                n_estimators = 200,\n",
    "                               )\n",
    "    Mdl_xgb_ODI = xgb.XGBClassifier(tree_method = 'hist',\n",
    "                                learning_rate = 0.01,\n",
    "                                #objective = 'binary:logistic',\n",
    "                                objective = 'multi:softmax',\n",
    "                                use_label_encoder = False,\n",
    "                                verbosity = 0,\n",
    "                                max_depth = 6,\n",
    "                                n_estimators = 200,\n",
    "                               )\n",
    "    \n",
    "    Xtrain_ODI = train_ODI.drop(columns = ['OSA'])\n",
    "    Ytrain_ODI = train_ODI.iloc[:, -1]\n",
    "    Xtrain_AHI = train_AHI.drop(columns = ['OSA'])\n",
    "    Ytrain_AHI = train_AHI.iloc[:, -1]\n",
    "    \n",
    "    Xtrain_ODIsm, Ytrain_ODIsm = sm.fit_resample(Xtrain_ODI, Ytrain_ODI)\n",
    "    Xtrain_AHIsm, Ytrain_AHIsm = sm.fit_resample(Xtrain_AHI, Ytrain_AHI)\n",
    "    \n",
    "    Xtest_ODI = test_ODI.drop(columns = ['OSA'])\n",
    "    Ytest_ODI = test_ODI.iloc[:, -1]\n",
    "    Xtest_AHI = test_AHI.drop(columns = ['OSA'])\n",
    "    Ytest_AHI = test_AHI.iloc[:, -1]\n",
    "    \n",
    "    #Mdl_xgb_ODI.fit(Xtrain_ODI, Ytrain_ODI)\n",
    "    #Mdl_xgb_AHI.fit(Xtrain_AHI, Ytrain_AHI)\n",
    "    \n",
    "    Mdl_xgb_ODI.fit(Xtrain_ODIsm, Ytrain_ODIsm)\n",
    "    Mdl_xgb_AHI.fit(Xtrain_AHIsm, Ytrain_AHIsm)\n",
    "    \n",
    "    predictions_xgb_ODI = Mdl_xgb_ODI.predict(Xtest_ODI)\n",
    "    predictions_xgb_AHI = Mdl_xgb_AHI.predict(Xtest_AHI)\n",
    "    \n",
    "    mat_predictions_xgb_AHI[k, :] = Evaluation(predictions_xgb_AHI, 3)\n",
    "    mat_predictions_xgb_ODI[k, :] = Evaluation(predictions_xgb_ODI, 3)\n",
    "        \n",
    "    xgb_osa_eval_AHI = OSAEvaluation(predictions_xgb_AHI, Ytest_AHI.values)\n",
    "    xgb_osa_eval_ODI = OSAEvaluation(predictions_xgb_ODI, Ytest_ODI.values)\n",
    "\n",
    "    scores_xgb[k, 0] = accuracy_score(Ytest_AHI, predictions_xgb_AHI)\n",
    "    scores_xgb[k, 1] = f1_score(Ytest_AHI, predictions_xgb_AHI, labels = np.unique(predictions_xgb_AHI), average = 'macro')\n",
    "    scores_xgb[k, 2] = sum(xgb_osa_eval_AHI) / len(xgb_osa_eval_AHI)\n",
    "    scores_xgb[k, 3] = accuracy_score(Ytest_ODI, predictions_xgb_ODI)\n",
    "    scores_xgb[k, 4] = f1_score(Ytest_ODI, predictions_xgb_ODI, labels = np.unique(predictions_xgb_ODI), average = 'macro')\n",
    "    scores_xgb[k, 5] = sum(xgb_osa_eval_ODI) / len(xgb_osa_eval_ODI)\n",
    "    #scores_xgb[k, 6] = recall_score(Ytest_AHI, predictions_xgb_AHI, labels = np.arange(n_class), average = 'macro')\n",
    "    #scores_xgb[k, 7] = recall_score(Ytest_ODI, predictions_xgb_ODI, labels = np.arange(n_class), average = 'macro')\n",
    "    \n",
    "    predictions_AHI = np.concatenate([predictions_AHI, predictions_xgb_AHI])\n",
    "    predictions_ODI = np.concatenate([predictions_ODI, predictions_xgb_ODI])\n",
    "    true_AHI = np.concatenate([true_AHI, Ytest_AHI])\n",
    "    true_ODI = np.concatenate([true_ODI, Ytest_ODI])\n",
    "    \n",
    "    features_xgb_train_AHI[k, :] = Mdl_xgb_AHI.feature_importances_\n",
    "    features_xgb_train_ODI[k, :] = Mdl_xgb_ODI.feature_importances_\n",
    "\n",
    "    #CM_AHI += confusion_matrix(Ytest_AHI, predictions_xgb_AHI, labels = np.arange(n_classOSA))\n",
    "    #CM_ODI += confusion_matrix(Ytest_ODI, predictions_xgb_ODI, labels = np.arange(n_classOSA))\n",
    "    \n",
    "    #    AUC_AHI[i, :] = roc_auc_score(Ytest_AHI, predictions_xgb_AHI)\n",
    "    #    AUC_ODI[i, :] = roc_auc_score(Ytest_ODI, predictions_xgb_ODI)\n",
    "    \n",
    "    finish = time.time()\n",
    "\n",
    "    #del Mdl_xgb_AHI, Mdl_xgb_ODI\n",
    "    print(finish - start)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dff22195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy AHI:  0.5512040111842544\n",
      "Mean F1 AHI:  0.3188367318695015\n",
      "Mean F1 AHI:  0.7515206342006021\n",
      "Mean Accuracy ODI:  0.5402461202270822\n",
      "Mean F1 ODI:  0.3347460032121691\n",
      "Mean F1 ODI:  0.821259661426896\n"
     ]
    }
   ],
   "source": [
    "scores_pdxgb = pd.DataFrame(scores_xgb, columns = ['Accuracy AHI', 'F1-Macro AHI', 'AHI OSA-Specific', \n",
    "                                                'Accuracy ODI', 'F1-Macro ODI', 'ODI OSA-Specific', \n",
    "                                                'Recall AHI', 'Recall ODI'])\n",
    "print('Mean Accuracy AHI: ', np.mean(scores_pdxgb['Accuracy AHI']))\n",
    "print('Mean F1 AHI: ', np.mean(scores_pdxgb['F1-Macro AHI']))\n",
    "print('Mean F1 AHI: ', np.mean(scores_pdxgb['AHI OSA-Specific']))\n",
    "\n",
    "print('Mean Accuracy ODI: ', np.mean(scores_pdxgb['Accuracy ODI']))\n",
    "print('Mean F1 ODI: ', np.mean(scores_pdxgb['F1-Macro ODI']))\n",
    "print('Mean F1 ODI: ', np.mean(scores_pdxgb['ODI OSA-Specific']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "84f4a898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy AHI:  0.5374912650964238\n",
      "Mean F1 AHI:  0.2603764160073899\n",
      "Mean F1 AHI:  0.8185263591944395\n",
      "Mean Accuracy ODI:  0.6293998837848069\n",
      "Mean F1 ODI:  0.3294670082613276\n",
      "Mean F1 ODI:  0.8433539873813231\n"
     ]
    }
   ],
   "source": [
    "scores_pdxgb = pd.DataFrame(scores_xgb, columns = ['Accuracy AHI', 'F1-Macro AHI', 'AHI OSA-Specific', \n",
    "                                                'Accuracy ODI', 'F1-Macro ODI', 'ODI OSA-Specific', \n",
    "                                                'Recall AHI', 'Recall ODI'])\n",
    "print('Mean Accuracy AHI: ', np.mean(scores_pdxgb['Accuracy AHI']))\n",
    "print('Mean F1 AHI: ', np.mean(scores_pdxgb['F1-Macro AHI']))\n",
    "print('Mean F1 AHI: ', np.mean(scores_pdxgb['AHI OSA-Specific']))\n",
    "\n",
    "print('Mean Accuracy ODI: ', np.mean(scores_pdxgb['Accuracy ODI']))\n",
    "print('Mean F1 ODI: ', np.mean(scores_pdxgb['F1-Macro ODI']))\n",
    "print('Mean F1 ODI: ', np.mean(scores_pdxgb['ODI OSA-Specific']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0894f059",
   "metadata": {},
   "outputs": [],
   "source": [
    "CM_test_sum = np.sum(CM_test, axis = 1, keepdims = True)\n",
    "CM_test_norm = CM_test.astype('float') / CM_test.sum(axis = 1)[:, np.newaxis]\n",
    "\n",
    "#CM_AHI_sum = np.sum(CM_ODI, axis = 1, keepdims = True)\n",
    "#CM_AHI_norm = CM_ODI.astype('float') / CM_ODI.sum(axis = 1)[:, np.newaxis]\n",
    "\n",
    "plt.rcParams['mathtext.fontset'] = 'stix'\n",
    "plt.rcParams['font.family'] = 'STIXGeneral'\n",
    "plt.rcParams['xtick.labelsize'] = 28\n",
    "plt.rcParams['ytick.labelsize'] = 28\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12, 10), dpi = 300)\n",
    "\n",
    "sns.set(rc={'text.usetex': True})\n",
    "sns.heatmap(CM_test_norm, \n",
    "            annot = True, \n",
    "            fmt = '.2f', \n",
    "            xticklabels = classes, \n",
    "            yticklabels = classes,\n",
    "            cbar = False,\n",
    "            annot_kws = {'size' : 32}\n",
    "           )\n",
    "plt.setp(ax.get_yticklabels(), rotation = 0, ha = \"right\",\n",
    "         rotation_mode = \"anchor\")\n",
    "plt.xlabel('Predicted sleep stages', fontsize = 28)\n",
    "plt.ylabel('True sleep stages', fontsize = 28)\n",
    "#plt.savefig(r'C:\\Users\\Jacopo\\Desktop\\Paper - EDA Sleep Staging\\Frontiers Final\\Revision\\PlotsRevision\\CM5_uni.eps', format = 'eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3ff03cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "CM_test2_sum = np.sum(CM_test2, axis = 1, keepdims = True)\n",
    "CM_test2_norm = CM_test2.astype('float') / CM_test2.sum(axis = 1)[:, np.newaxis]\n",
    "\n",
    "plt.rcParams['mathtext.fontset'] = 'stix'\n",
    "plt.rcParams['font.family'] = 'STIXGeneral'\n",
    "plt.rcParams['xtick.labelsize'] = 28\n",
    "plt.rcParams['ytick.labelsize'] = 28\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12, 10), dpi = 300)\n",
    "\n",
    "sns.set(rc={'text.usetex': True})\n",
    "sns.heatmap(CM_test2_norm, \n",
    "            annot = True, \n",
    "            fmt = '.2f', \n",
    "            xticklabels = classes, \n",
    "            yticklabels = classes,\n",
    "            cbar = False,\n",
    "            annot_kws = {'size' : 32}\n",
    "           )\n",
    "plt.setp(ax.get_yticklabels(), rotation = 0, ha = \"right\",\n",
    "         rotation_mode = \"anchor\")\n",
    "plt.xlabel('Predicted sleep stages', fontsize = 28)\n",
    "plt.ylabel('True sleep stages', fontsize = 28)\n",
    "#plt.savefig(r'C:\\Users\\Jacopo\\Desktop\\Paper - EDA Sleep Staging\\Frontiers Final\\Revision\\PlotsRevision\\CM5_pers.eps', format = 'eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c34faa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.DataFrame(data = Xn,\n",
    "                 columns = np.arange(1, 1 + Xnpd.shape[1]))\n",
    "cor_matrix = d.corr().abs()\n",
    "tri_matrix = cor_matrix.where(np.triu(np.ones(cor_matrix.shape), k = 1).astype(bool))\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap = True)\n",
    "mask = np.triu(np.ones_like(cor_matrix, dtype=bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37c07cf7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(16, 14))\n",
    "\n",
    "sns.set(rc={'text.usetex': True})\n",
    "sns.set_theme(style=\"white\")\n",
    "sns.heatmap(cor_matrix, \n",
    "            xticklabels = cor_matrix.columns, \n",
    "            yticklabels=  cor_matrix.columns,\n",
    "            mask = mask, \n",
    "            cmap = cmap,  \n",
    "            center = 0,\n",
    "            square = True, \n",
    "            linewidths = .01, \n",
    "            #cbar = False,\n",
    "            cbar_kws = {\"shrink\": 0.5}\n",
    "           )\n",
    "plt.setp(ax.get_yticklabels(), rotation = 0, ha = \"right\",\n",
    "         rotation_mode = \"anchor\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "379c4377",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "[2, 3, 5, 6, 9, 11, 14, 15, 18, 24, 26, 28, 30, 31, 32, 34, 38, 41, 42, 43, 45, 47, 48, 49, 51, 53, 54, 55, 57, 59, 60, 61, 63, 66, 67, 69, 70]\n"
     ]
    }
   ],
   "source": [
    "dd = d.corr().abs()\n",
    "upper_tri = dd.where(np.triu(np.ones(dd.shape), k = 1).astype(bool))\n",
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > .8)]\n",
    "print(len(to_drop))\n",
    "print(to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0fbc96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xysrpd = d.drop(columns = to_drop, axis = 1)\n",
    "cor = Xysrpd.corr()\n",
    "tri_matrix = cor.where(np.triu(np.ones(cor.shape), k = 1).astype(bool))\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap = True)\n",
    "mask2 = np.triu(np.ones_like(cor, dtype=bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af45a4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(16, 14))\n",
    "\n",
    "sns.set(rc={'text.usetex': True})\n",
    "sns.set_theme(style=\"white\")\n",
    "sns.heatmap(cor, \n",
    "            xticklabels = cor.columns, \n",
    "            yticklabels=  cor.columns, \n",
    "            mask = mask2, \n",
    "            cmap = cmap,  \n",
    "            center = 0,\n",
    "            square = True, \n",
    "            linewidths = .01, \n",
    "            #cbar = False,\n",
    "            cbar_kws = {\"shrink\": 0.5}\n",
    "           )\n",
    "plt.setp(ax.get_yticklabels(), rotation = 0, ha = \"right\",\n",
    "         rotation_mode = \"anchor\")\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
